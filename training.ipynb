{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb6c69f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%cd /content\n",
    "!rm -rf DeepLearningSystem_CMU10414\n",
    "!git clone https://github.com/yyj6666667/DeepLearningSystem_CMU10414\n",
    "%cd DeepLearningSystem_CMU10414/hw4\n",
    "\n",
    "\n",
    "!pip3 install pybind11\n",
    "\n",
    "!make clean\n",
    "!make\n",
    "\n",
    "%set_env PYTHONPATH ./python\n",
    "%set_env NEEDLE_BACKEND nd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22866d21",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "!mkdir -p './data/ptb'\n",
    "# Download Penn Treebank dataset\n",
    "ptb_data = \"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.\"\n",
    "for f in ['train.txt', 'test.txt', 'valid.txt']:\n",
    "    if not os.path.exists(os.path.join('./data/ptb', f)):\n",
    "        urllib.request.urlretrieve(ptb_data + f, os.path.join('./data/ptb', f))\n",
    "\n",
    "# Download CIFAR-10 dataset\n",
    "if not os.path.isdir(\"./data/cifar-10-batches-py\"):\n",
    "    urllib.request.urlretrieve(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", \"./data/cifar-10-python.tar.gz\")\n",
    "    !tar -xvzf './data/cifar-10-python.tar.gz' -C './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e696f",
   "metadata": {},
   "source": [
    "以下用于快速验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d9d162",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sys.path.append(\"/python\")\n",
    "from needle import cpu, data, nn, optim, Tensor\n",
    "np.random.seed(0)\n",
    "device = cpu()\n",
    "dataset = data.CIFAR10Dataset(\"./data/cifar-10-batches-py\", train=True)\n",
    "dataloader = data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "from apps.models import ResNet9\n",
    "np.random.seed(0)\n",
    "\n",
    "def one_iter_of_cifar10_training(dataloader, model, niter=1, loss_fn=None, opt=None):\n",
    "    if loss_fn is None:\n",
    "        loss_fn = nn.SoftmaxLoss()\n",
    "    if opt is None:\n",
    "        opt = optim.Adam(model.parameters())\n",
    "    \n",
    "    np.random.seed(4)\n",
    "    model.train()\n",
    "    total_correct, total_loss, total_samples = 0, 0, 0\n",
    "    i = 0\n",
    "    for batch in dataloader:\n",
    "        if i >= niter: \n",
    "            break\n",
    "        opt.reset_grad()\n",
    "        X, y = batch\n",
    "        X, y = Tensor(X, device=device), Tensor(y, device=device)\n",
    "        print(\"X, y is loaded into cpu\")\n",
    "        out = model(X)\n",
    "        print(f\"X range: [{X.numpy().min()}, {X.numpy().max()}]\")\n",
    "        print(f\"Model output range: [{out.numpy().min()}, {out.numpy().max()}]\")\n",
    "\n",
    "        cur_correct = np.sum(np.argmax(out.numpy(), axis=1) == y.numpy())\n",
    "        total_correct += cur_correct\n",
    "        total_samples += y.shape[0]\n",
    "\n",
    "        loss = loss_fn(out, y)\n",
    "        total_loss += loss.data.numpy() * y.shape[0]\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        i += 1\n",
    "        batch_cor = cur_correct / (y.shape[0])\n",
    "        print(f\"for batch{i}: batch_correct is {batch_cor}, batch_loss is {loss}\")\n",
    "    return total_correct/(total_samples), total_loss/total_samples\n",
    "\n",
    "model = ResNet9(device=device, dtype=\"float32\")\n",
    "out = one_iter_of_cifar10_training(\n",
    "    dataloader, \n",
    "    model, \n",
    "    niter = 10,\n",
    "    opt=optim.Adam(model.parameters(), lr=0.000005, weight_decay=0.01)\n",
    ")\n",
    "print(f\"one epoch: correct is {out[0]}, total_loss is {out[1]}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
